{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UKOQcXNtJ8e"
   },
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **Algorithms for machine learning and inference** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Linear and Bayesian Regression**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Arman Rahbar** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 27 April** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Name, Personal no., email** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQ8gZxqWtJ8h"
   },
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical and pratical problems must be submitted in this ipynb notebook, and equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. \n",
    "All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.\n",
    "* Your name, personal number and email address should be specified above.\n",
    "* All tables and other additional information should be included in this notebook.\n",
    "* ** Before submitting, make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vrnQ98KgtJ8i"
   },
   "source": [
    "**Jupyter/IPython Notebook** is a collaborative Python web-based environment. This will be used in all our Homework Assignments. It is installed in the halls ES61-ES62, E-studio and MT9. You can also use google-colab: https://colab.research.google.com\n",
    "to run these notebooks without having to download, install, or do anything on your own computer other than a browser.\n",
    "Some useful resources:\n",
    "1. https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/ (Quick-start guide)\n",
    "2. https://www.kdnuggets.com/2016/04/top-10-ipython-nb-tutorials.html\n",
    "3. http://data-blog.udacity.com/posts/2016/10/latex-primer/ (latex-primer)\n",
    "4. http://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html (markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xlDcjE-FZc_o"
   },
   "source": [
    "# [Linear Regression with regularization 11pt]\n",
    "\n",
    "You are newly recruited as a Data Scientist at a leading consultancy company in Gothenburg. Your first task at the job is to help the Swedish Public Health Agency (folkhalsomyndigheten) for predicting the diabetes progression of patients. Assume that you are given a dataset D of $n$ patients with $D = \\{ (\\mathbf{x}_i, y_i)\\}_{i=1}^n$ where $\\mathbf{x}_i \\in \\mathbb{R}^p$ represents numerical features of each patients and $y_i \\in \\mathbb{R}$ represent the numerical diabetes progression.  One can also view the dataset D as a pair of matrices $(\\mathbf{X}, \\mathbf{y})$ with $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ and $\\mathbf{y} \\in \\mathbb{R}^{n \\times 1}$.\n",
    "\n",
    "Fresh with the lectures in the machine learning course at Chalmers, you would like to use a linear model to quickly perform the task. In order words, you would like to find a vector $\\mathbf{w} \\in \\mathbb{R}^{p \\times 1}$  such that $\\mathbf{y} = \\mathbf{X} \\mathbf{w}$.  However,  you have just read one of the most popular machine learning book and it argues that standard linear regression (for finding $\\mathbf{w}$) can lead to various problems such as non-uniqueness of the solution,  overfitting, etc. As a result, you decided to add a penalty term called regularization to control the optimisation problem. More specifically, you want to solve for: $\\min_{\\mathbf{w}}  \\mathcal{L}(\\mathbf{w})$ where  $\\mathcal{L}(\\mathbf{w}) = \\left(\\sum_{i=1}^n (y_i - \\mathbf{w}^T\\mathbf{x}_i)^2 \\right) + \\left(\\alpha \\sum_{j=1}^p w_j^2 \\right) $ with $\\alpha \\in \\mathbb{R}$ a small coefficient that you will decide later on.\n",
    "\n",
    "1-  **1pt** Write down $\\mathcal{L}(\\mathbf{w})$ in matrix/vector forms using only $\\mathbf{X}$, $\\mathbf{y}$ and $\\mathbf{w}$ and the L2 norm. In other words, you are not allowed to use any components $y_i, \\mathbf{w}_j$ or $\\mathbf{x}_i$ ( For any vector $\\mathbf{z}$ use the following notation $|\\mathbf{z}|_2$ to mean the L2 norm of  $\\mathbf{z}$ See http://mathworld.wolfram.com/L2-Norm.html for more information about the L2 norm.)\n",
    "\n",
    "2- **1pt** Derive and write down in matrix/vector forms the gradient of $\\mathcal{L}(\\mathbf{w})$ with respect to $\\mathbf{w}$. Show all the derivations. (Hint: You can start by  computing the gradient of the full expression and then convert it to matrix/vector forms. You can also directly get the gradients from your answer in 1-)\n",
    "\n",
    "\n",
    "3- **2pt** Derive and write down in matrix/vector forms the solution $\\mathbf{w}^*$ to the optimization problem $\\min_{\\mathbf{w}}  \\mathcal{L}(\\mathbf{w})$. Show all your derivations. (Hint: $\\mathcal{L}(\\mathbf{w})$ is convex in $\\mathbf{w}$)\n",
    "\n",
    "4-  **2pt** Under which condition on the $\\alpha$ is the solution $\\mathbf{w}^*$ unique? Prove rigorously your statement. Make no assumptions on $\\mathbf{X}$. (Hint: If your solution $\\mathbf{w}^*$ requires to invert a matrix, then one necessary condition for uniquess is for the matrix to be invertible. And any positive definitive matrix https://en.wikipedia.org/wiki/Definiteness_of_a_matrix is invertible. You might also want to look at the properties of transposition https://en.wikipedia.org/wiki/Transpose)\n",
    "\n",
    "5- **2pt** Implement in Python a well commented function **fit_linear_with_regularization** that takes as input $\\mathbf{X}$, $\\mathbf{y}$ and $\\alpha$ and return $\\mathbf{w}^*$ as computed in question 3. You are not allowed to use any loops (for-loop, while-loop ...) to do the implementation. Instead use and abuse as much as possible numpy vectorization techniques. A skeleton of the function is shown in the code cell below.\n",
    "\n",
    "6- **3pt** Implement in Python a well commented function **predict** that takes as input a dataset $\\mathbf{X_{\\text{test}}}$ in the same dimensions as $\\mathbf{X}$ and return the predictions.   Write down the mean squared error (https://en.wikipedia.org/wiki/Mean_squared_error) of your predictions. Then on the same plot with legends, show:\n",
    "\n",
    " a) A scatter plot of the first feature of $\\mathbf{X_{\\text{test}}}$ (x-axis) and the diabetes progression $\\mathbf{y_{\\text{test}}}$ \n",
    " \n",
    " b) A plot of your prediction for $\\mathbf{X_{\\text{test}}}$\n",
    " \n",
    " The skeleton code in the cell below already implements most of data loading and you should only have to fill in the *TODO* part. Again here no loops are allowed (for-loop, while loop in the implementation of the plots and the **predict** )\n",
    " \n",
    " **Bonus question**\n",
    " \n",
    "In the code from the cell below, we use alpha = 0.01. If you want, you could check if you can improve the mean squared error by using a different value of strictly positive alpha  and/or by normalizing the input features $\\mathbf{X}$. \n",
    "\n",
    "Finding this improvement, will not give you any additional points. Not finding an improvement  will not prevent you from getting full points (if all questions with points are answered correctly). However, if you find an improvement, we will pick exactly one question where you didn't receive full points in this assignment and give you full points there. In particular,  between the questions for which you have reasonably attempted a solution, we will pick the one where the difference between the full point and the point you received is the maximum.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** \n",
    "$\\mathcal{L}(\\mathbf{w}) = (\\mathbf{y} - \\mathbf{Xw})^T(\\mathbf{y} - \\mathbf{Xw}) + \\alpha\\cdot\\mathbf{w}^T\\mathbf{w}$\n",
    "\n",
    "**2.**\n",
    "$\\dfrac{\\partial\\mathcal{L}(\\mathbf{w})}{\\partial\\mathbf{w}} = -2\\mathbf{X}^T(\\mathbf{y - Xw}) + 2\\alpha\\mathbf{w}$\n",
    "\n",
    "**3.**\n",
    "Let\n",
    "$$\\dfrac{\\partial\\mathcal{L}(\\mathbf{w})}{\\partial\\mathbf{w}} = -2\\mathbf{X}^T(\\mathbf{y - Xw}) + 2\\alpha\\mathbf{w} = 0$$\n",
    "we have\n",
    "\\begin{equation}\n",
    "\\alpha\\mathbf{w} = \\mathbf{X^Ty} - \\mathbf{X^TXw} \\implies (\\alpha\\mathbf{I_p} + \\mathbf{X^TX})\\mathbf{w} = \\mathbf{X^Ty} \\\\\n",
    "\\mathbf{w^*} = (\\alpha\\mathbf{I_p} + \\mathbf{X^TX})^{-1}\\mathbf{X^Ty}\n",
    "\\end{equation}\n",
    "\n",
    "**4.**\n",
    "As hinted by the question, in order for $\\mathbf{w^*}$ to be unique, matrix $\\alpha\\mathbf{I_p} + \\mathbf{X^TX}$ should be invertible. We try to find the condition of $\\mathbf{w^*}$ under which this is always a positive definite matrix. To do so, assume $\\mathbf{z}$ be any $p$ dimension vector, we have\n",
    "$$\\mathbf{z^T}(\\alpha\\mathbf{I_p} + \\mathbf{X^TX})\\mathbf{z} = \\alpha\\mathbf{z^Tz} + \\mathbf{z^TX^TXz} \n",
    "= \\alpha(|\\mathbf{z}|_2)^2 + (|\\mathbf{Xz}|_2)^2$$\n",
    "Since $(|\\mathbf{z}|_2)^2 > 0$ and $(|\\mathbf{Xz}|_2)^2 \\ge 0$, by the definition of positive definite matrix, $\\alpha$ should satisfy\n",
    "$$\\alpha \\gt 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03807591 -0.00188202  0.08529891 -0.08906294  0.00538306 -0.09269548\n",
      " -0.04547248  0.06350368  0.04170844 -0.07090025 -0.09632802  0.02717829\n",
      "  0.01628068  0.00538306  0.04534098 -0.05273755 -0.00551455  0.07076875\n",
      " -0.0382074  -0.02730979 -0.04910502 -0.0854304  -0.0854304   0.04534098\n",
      " -0.06363517 -0.06726771 -0.10722563 -0.02367725  0.05260606  0.06713621\n",
      " -0.06000263 -0.02367725  0.03444337  0.03081083  0.01628068  0.04897352\n",
      "  0.01264814 -0.00914709 -0.00188202 -0.00188202  0.00538306 -0.09996055\n",
      " -0.06000263  0.01991321  0.04534098  0.02717829 -0.05637009 -0.07816532\n",
      "  0.06713621 -0.04183994  0.03444337  0.05987114 -0.05273755 -0.00914709\n",
      " -0.04910502 -0.04183994 -0.04183994 -0.02730979  0.04170844  0.06350368\n",
      " -0.07090025 -0.04183994 -0.02730979 -0.03457486  0.06713621 -0.04547248\n",
      " -0.00914709  0.04170844  0.03807591  0.01628068 -0.00188202 -0.00188202\n",
      "  0.06350368  0.01264814  0.01264814 -0.00914709 -0.03094232 -0.09632802\n",
      "  0.00538306 -0.10359309  0.07076875  0.01264814 -0.01641217 -0.0382074\n",
      "  0.00175052  0.04534098 -0.07090025  0.04534098 -0.05273755 -0.04547248\n",
      "  0.01264814  0.04534098 -0.02004471 -0.04910502 -0.07816532 -0.07090025\n",
      "  0.0562386  -0.02730979  0.00175052 -0.00188202  0.01628068  0.01628068\n",
      " -0.09269548  0.05987114 -0.02730979  0.02354575 -0.09632802  0.02717829\n",
      "  0.01991321  0.03807591  0.04170844  0.01991321 -0.0854304   0.01991321\n",
      "  0.02354575 -0.03094232  0.04897352  0.05987114 -0.05637009  0.01628068\n",
      " -0.04910502  0.06350368  0.04897352  0.00538306 -0.00551455 -0.00551455\n",
      " -0.08906294  0.03444337 -0.05273755  0.0090156  -0.06363517 -0.09632802\n",
      "  0.01628068 -0.04183994 -0.07453279 -0.00551455 -0.09269548  0.00538306\n",
      "  0.03444337  0.02354575  0.04170844 -0.02730979  0.04170844 -0.03094232\n",
      "  0.03081083 -0.04183994 -0.03094232 -0.05637009 -0.06000263 -0.04910502\n",
      "  0.02354575  0.00175052 -0.03457486  0.04170844  0.06713621 -0.02730979\n",
      " -0.01641217 -0.00188202 -0.01277963 -0.00551455 -0.00914709 -0.04547248\n",
      " -0.05273755  0.01628068  0.04534098 -0.04183994 -0.05637009  0.07076875\n",
      "  0.00175052 -0.00188202  0.02354575 -0.02004471  0.04170844 -0.06363517\n",
      "  0.01628068  0.06713621  0.04534098  0.04897352  0.04170844 -0.02367725\n",
      " -0.0382074   0.04897352  0.04534098  0.04534098  0.01628068 -0.07453279\n",
      " -0.08179786 -0.06726771  0.00538306 -0.00188202  0.0090156  -0.00551455\n",
      "  0.0562386   0.0090156  -0.06726771  0.02717829 -0.02367725  0.04897352\n",
      " -0.05273755  0.04170844  0.0562386  -0.03457486  0.08166637 -0.00188202\n",
      "  0.11072668 -0.03094232  0.00175052  0.0090156   0.03081083  0.03807591\n",
      "  0.0090156   0.09256398  0.06713621  0.00175052  0.03081083  0.02717829\n",
      "  0.01264814  0.07440129 -0.04183994 -0.08906294  0.02354575 -0.04547248\n",
      " -0.02367725 -0.09996055 -0.02730979  0.03081083 -0.10359309  0.06713621\n",
      " -0.05273755 -0.02730979 -0.0382074   0.0090156   0.01264814  0.06713621\n",
      "  0.04534098  0.06713621  0.02717829  0.0562386   0.03444337  0.02354575\n",
      "  0.04897352  0.03081083 -0.10359309  0.01628068 -0.06000263 -0.02730979\n",
      "  0.04170844 -0.08179786 -0.04183994 -0.01277963  0.06713621 -0.05273755\n",
      "  0.00538306  0.08166637  0.03081083  0.00175052 -0.04910502 -0.02730979\n",
      "  0.07803383  0.01264814  0.04170844  0.04897352 -0.01641217 -0.07453279\n",
      "  0.03444337 -0.03457486 -0.05273755  0.05987114  0.06350368  0.0090156\n",
      "  0.00538306  0.03807591  0.01264814  0.07440129  0.01628068 -0.00551455\n",
      "  0.01264814 -0.03457486  0.06713621  0.03807591  0.0090156  -0.09269548\n",
      "  0.07076875 -0.01641217  0.04170844  0.01264814 -0.0382074   0.04534098\n",
      "  0.07076875 -0.07453279  0.05987114  0.07440129  0.0090156  -0.07090025\n",
      "  0.02354575 -0.05273755  0.06713621  0.00175052  0.02354575  0.03807591\n",
      "  0.01628068 -0.00188202  0.01264814  0.07440129  0.04170844 -0.00914709\n",
      "  0.0090156   0.06713621  0.00175052 -0.00914709 -0.00551455  0.09619652\n",
      " -0.07453279  0.05987114 -0.02367725  0.0090156   0.01628068  0.01991321\n",
      "  0.08893144  0.01991321 -0.02367725  0.09619652  0.02354575  0.07076875\n",
      "  0.03081083 -0.00188202  0.04534098  0.07440129 -0.0382074  -0.01277963\n",
      "  0.0090156   0.08166637  0.03081083  0.02717829 -0.06000263  0.00538306\n",
      " -0.02004471  0.01991321 -0.06363517  0.02717829 -0.01641217  0.03081083\n",
      "  0.0562386  -0.02004471 -0.10722563  0.08166637  0.00538306  0.03807591\n",
      "  0.03081083  0.00175052 -0.02730979 -0.0854304   0.01264814 -0.05273755\n",
      " -0.02367725 -0.07453279 -0.00551455 -0.06000263 -0.02004471  0.03807591\n",
      "  0.01628068  0.04170844  0.01991321 -0.04910502  0.00175052  0.03444337\n",
      " -0.04547248 -0.00914709 -0.01641217 -0.00914709  0.01991321  0.05260606\n",
      " -0.02730979 -0.07453279 -0.10722563  0.04534098 -0.00188202  0.01991321\n",
      "  0.01628068 -0.00188202  0.01628068 -0.07090025  0.04897352  0.00538306\n",
      "  0.03444337  0.02354575  0.01991321 -0.04547248  0.05260606 -0.00551455\n",
      "  0.0090156  -0.02367725 -0.04183994 -0.07453279  0.03444337 -0.06000263\n",
      " -0.0854304   0.05260606  0.01264814  0.05987114 -0.02367725  0.01628068\n",
      "  0.11072668 -0.02004471 -0.01641217  0.04897352 -0.05637009  0.02717829\n",
      "  0.06350368 -0.05273755 -0.00914709  0.00538306  0.07440129 -0.05273755\n",
      "  0.08166637 -0.00551455 -0.02730979 -0.05273755  0.0090156  -0.02004471\n",
      "  0.02354575  0.03807591 -0.07816532  0.0090156   0.00175052 -0.07816532\n",
      "  0.03081083 -0.03457486  0.04897352 -0.04183994 -0.00914709  0.07076875\n",
      "  0.0090156  -0.02730979  0.01628068 -0.01277963 -0.05637009  0.04170844\n",
      " -0.00551455  0.04170844 -0.04547248 -0.04547248]\n",
      "[ 0.03807591 -0.00188202  0.08529891 -0.08906294  0.00538306 -0.09269548\n",
      " -0.04547248  0.06350368  0.04170844 -0.07090025 -0.09632802  0.02717829\n",
      "  0.01628068  0.00538306  0.04534098 -0.05273755 -0.00551455  0.07076875\n",
      " -0.0382074  -0.02730979 -0.04910502 -0.0854304  -0.0854304   0.04534098\n",
      " -0.06363517 -0.06726771 -0.10722563 -0.02367725  0.05260606  0.06713621\n",
      " -0.06000263 -0.02367725  0.03444337  0.03081083  0.01628068  0.04897352\n",
      "  0.01264814 -0.00914709 -0.00188202 -0.00188202  0.00538306 -0.09996055\n",
      " -0.06000263  0.01991321  0.04534098  0.02717829 -0.05637009 -0.07816532\n",
      "  0.06713621 -0.04183994  0.03444337  0.05987114 -0.05273755 -0.00914709\n",
      " -0.04910502 -0.04183994 -0.04183994 -0.02730979  0.04170844  0.06350368\n",
      " -0.07090025 -0.04183994 -0.02730979 -0.03457486  0.06713621 -0.04547248\n",
      " -0.00914709  0.04170844  0.03807591  0.01628068 -0.00188202 -0.00188202\n",
      "  0.06350368  0.01264814  0.01264814 -0.00914709 -0.03094232 -0.09632802\n",
      "  0.00538306 -0.10359309  0.07076875  0.01264814 -0.01641217 -0.0382074\n",
      "  0.00175052  0.04534098 -0.07090025  0.04534098 -0.05273755 -0.04547248\n",
      "  0.01264814  0.04534098 -0.02004471 -0.04910502 -0.07816532 -0.07090025\n",
      "  0.0562386  -0.02730979  0.00175052 -0.00188202  0.01628068  0.01628068\n",
      " -0.09269548  0.05987114 -0.02730979  0.02354575 -0.09632802  0.02717829\n",
      "  0.01991321  0.03807591  0.04170844  0.01991321 -0.0854304   0.01991321\n",
      "  0.02354575 -0.03094232  0.04897352  0.05987114 -0.05637009  0.01628068\n",
      " -0.04910502  0.06350368  0.04897352  0.00538306 -0.00551455 -0.00551455\n",
      " -0.08906294  0.03444337 -0.05273755  0.0090156  -0.06363517 -0.09632802\n",
      "  0.01628068 -0.04183994 -0.07453279 -0.00551455 -0.09269548  0.00538306\n",
      "  0.03444337  0.02354575  0.04170844 -0.02730979  0.04170844 -0.03094232\n",
      "  0.03081083 -0.04183994 -0.03094232 -0.05637009 -0.06000263 -0.04910502\n",
      "  0.02354575  0.00175052 -0.03457486  0.04170844  0.06713621 -0.02730979\n",
      " -0.01641217 -0.00188202 -0.01277963 -0.00551455 -0.00914709 -0.04547248\n",
      " -0.05273755  0.01628068  0.04534098 -0.04183994 -0.05637009  0.07076875\n",
      "  0.00175052 -0.00188202  0.02354575 -0.02004471  0.04170844 -0.06363517\n",
      "  0.01628068  0.06713621  0.04534098  0.04897352  0.04170844 -0.02367725\n",
      " -0.0382074   0.04897352  0.04534098  0.04534098  0.01628068 -0.07453279\n",
      " -0.08179786 -0.06726771  0.00538306 -0.00188202  0.0090156  -0.00551455\n",
      "  0.0562386   0.0090156  -0.06726771  0.02717829 -0.02367725  0.04897352\n",
      " -0.05273755  0.04170844  0.0562386  -0.03457486  0.08166637 -0.00188202\n",
      "  0.11072668 -0.03094232  0.00175052  0.0090156   0.03081083  0.03807591\n",
      "  0.0090156   0.09256398  0.06713621  0.00175052  0.03081083  0.02717829\n",
      "  0.01264814  0.07440129 -0.04183994 -0.08906294  0.02354575 -0.04547248\n",
      " -0.02367725 -0.09996055 -0.02730979  0.03081083 -0.10359309  0.06713621\n",
      " -0.05273755 -0.02730979 -0.0382074   0.0090156   0.01264814  0.06713621\n",
      "  0.04534098  0.06713621  0.02717829  0.0562386   0.03444337  0.02354575\n",
      "  0.04897352  0.03081083 -0.10359309  0.01628068 -0.06000263 -0.02730979\n",
      "  0.04170844 -0.08179786 -0.04183994 -0.01277963  0.06713621 -0.05273755\n",
      "  0.00538306  0.08166637  0.03081083  0.00175052 -0.04910502 -0.02730979\n",
      "  0.07803383  0.01264814  0.04170844  0.04897352 -0.01641217 -0.07453279\n",
      "  0.03444337 -0.03457486 -0.05273755  0.05987114  0.06350368  0.0090156\n",
      "  0.00538306  0.03807591  0.01264814  0.07440129  0.01628068 -0.00551455\n",
      "  0.01264814 -0.03457486  0.06713621  0.03807591  0.0090156  -0.09269548\n",
      "  0.07076875 -0.01641217  0.04170844  0.01264814 -0.0382074   0.04534098\n",
      "  0.07076875 -0.07453279  0.05987114  0.07440129  0.0090156  -0.07090025\n",
      "  0.02354575 -0.05273755  0.06713621  0.00175052  0.02354575  0.03807591\n",
      "  0.01628068 -0.00188202  0.01264814  0.07440129  0.04170844 -0.00914709\n",
      "  0.0090156   0.06713621  0.00175052 -0.00914709 -0.00551455  0.09619652\n",
      " -0.07453279  0.05987114 -0.02367725  0.0090156   0.01628068  0.01991321\n",
      "  0.08893144  0.01991321 -0.02367725  0.09619652  0.02354575  0.07076875\n",
      "  0.03081083 -0.00188202  0.04534098  0.07440129 -0.0382074  -0.01277963\n",
      "  0.0090156   0.08166637  0.03081083  0.02717829 -0.06000263  0.00538306\n",
      " -0.02004471  0.01991321 -0.06363517  0.02717829 -0.01641217  0.03081083\n",
      "  0.0562386  -0.02004471 -0.10722563  0.08166637  0.00538306  0.03807591\n",
      "  0.03081083  0.00175052 -0.02730979 -0.0854304   0.01264814 -0.05273755\n",
      " -0.02367725 -0.07453279 -0.00551455 -0.06000263 -0.02004471  0.03807591\n",
      "  0.01628068  0.04170844  0.01991321 -0.04910502  0.00175052  0.03444337\n",
      " -0.04547248 -0.00914709 -0.01641217 -0.00914709  0.01991321  0.05260606\n",
      " -0.02730979 -0.07453279 -0.10722563  0.04534098 -0.00188202  0.01991321\n",
      "  0.01628068 -0.00188202  0.01628068 -0.07090025  0.04897352  0.00538306\n",
      "  0.03444337  0.02354575  0.01991321 -0.04547248  0.05260606 -0.00551455\n",
      "  0.0090156  -0.02367725 -0.04183994 -0.07453279  0.03444337 -0.06000263\n",
      " -0.0854304   0.05260606  0.01264814  0.05987114 -0.02367725  0.01628068\n",
      "  0.11072668 -0.02004471 -0.01641217  0.04897352 -0.05637009  0.02717829\n",
      "  0.06350368 -0.05273755 -0.00914709  0.00538306  0.07440129 -0.05273755\n",
      "  0.08166637 -0.00551455 -0.02730979 -0.05273755  0.0090156  -0.02004471\n",
      "  0.02354575  0.03807591 -0.07816532  0.0090156   0.00175052 -0.07816532\n",
      "  0.03081083 -0.03457486  0.04897352 -0.04183994 -0.00914709  0.07076875\n",
      "  0.0090156  -0.02730979  0.01628068 -0.01277963 -0.05637009  0.04170844\n",
      " -0.00551455  0.04170844 -0.04547248 -0.04547248]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def fit_linear_with_regularization(X, y, alpha):\n",
    "    n, p = np.shape(X)  # Get number of data points and number of features\n",
    "    w = [None]*p        # Preallocate array of paramters\n",
    "    I = np.eye(p)       # Define identity matrix of dimension (p * p)\n",
    " \n",
    "    # Compute parameter vector w* according to the expression in task 3\n",
    "    w = np.matmul(inv(alpha*I+np.matmul(X.T,X)),np.matmul(X.T,y))\n",
    "    w0 = inv(alpha*I + X.T@X)@(X.T@y) # Alternative method using Python's matrix multiplication operator\n",
    "    return w0\n",
    "\n",
    "def predict(X_test, w):\n",
    "    return np.matmul(X_test, w)\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "G = np.hstack([ np.ones([X.shape[0], 1]), X])\n",
    "print(G[:, 1])\n",
    "print(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_qgzuqkuL-K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared error is  22050.708767162207\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 810x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make it possible to show plots in the notebooks.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fit_linear_with_regularization(X, y, alpha):\n",
    "    n, p = np.shape(X)  # Get number of data points and number of features\n",
    "    w = [None]*p        # Preallocate array of paramters\n",
    "    I = np.eye(p)       # Define identity matrix of dimension (p * p)\n",
    " \n",
    "    # Compute parameter vector w* according to the expression in task 3\n",
    "    w = np.matmul(inv(alpha * I + np.matmul(X.T,X)), np.matmul(X.T,y))\n",
    "    w0 = inv(alpha * I + X.T @ X) @ (X.T @ y) # Alternative method using Python's matrix multiplication operator\n",
    "    return w\n",
    "\n",
    "def predict(X_test, w):\n",
    "    return np.matmul(X_test, w)\n",
    "\n",
    "\n",
    "def plot_prediction(X_test, y_test, y_pred):\n",
    "    # Define figure and axes\n",
    "    plt.figure(num=None, figsize=(5.4, 4.0), dpi=150)\n",
    "    ax = plt.axes()\n",
    "    # Scatter plot the first feature of X_test (x-axis) and y_test (y-axis)\n",
    "    plt.scatter(X_test[:,0], y_test, label='y_test', marker='.')\n",
    "    # Plot y_pred using the first feature of X_test as x-axis\n",
    "    plt.scatter(X_test[:,0], y_pred, label='y_pred', marker='.')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    diff = y_test - y_pred\n",
    "    mean_squared_error = (diff @ diff.T) / len(diff)\n",
    "    return mean_squared_error\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Split the dataset into training and test set\n",
    "num_test_elements = 20\n",
    "\n",
    "X_train = X[:-num_test_elements]\n",
    "X_test = X[-num_test_elements:]\n",
    "\n",
    "y_train = y[:-num_test_elements]\n",
    "y_test = y[-num_test_elements:]\n",
    "\n",
    "# Set alpha\n",
    "alpha = 0.01\n",
    "\n",
    "# Train using linear regression with regularization and find optimal model\n",
    "w = fit_linear_with_regularization(X_train, y_train, alpha)\n",
    "\n",
    "\n",
    "# Make predictions using the testing set X_test\n",
    "y_pred = predict(X_test, w)\n",
    "\n",
    "\n",
    "# Plots and mean squared error\n",
    "error = plot_prediction(X_test, y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "print('Mean Squared error is ', error)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdmPgTGVzSmx"
   },
   "source": [
    "# Bayesian Linear Regression   4pt\n",
    "\n",
    "Proud of finishing the task using a linear model with regularization, you show your results to a representative of the Swedish Public Health Agency. You barely finish explaining your solution when the face of the representative turns red and you could distinctly hear: \"Bayesian is the only way: How come didn't you use any probabilities?\". \n",
    "\n",
    "You quickly head back to your desk and now assume a Gaussian prior on the solution $\\mathbf{w}$, that is $p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{0}, \\lambda^{-1} \\mathbf{I})$ where $\\lambda \\in \\mathbb{R}$ is a constant real number, $I$ is the $p \\times p$ identity matrix and $\\mathcal{N}(\\mathbf{0}, \\lambda^{-1} \\mathbf{I})$ is used to mean the multivariate gaussian distribution with mean $\\mathbf{0} \\in \\mathbb{R}^p$ , a vector of zeros of dimension $p$ and covariance matrix $\\lambda^{-1} \\mathbf{I}$ . Then, you use the following likelihood:\n",
    "\n",
    "$p(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}) = \\prod_{i=1}^n \\mathcal{N}(\\mathbf{w}^T \\mathbf{x}_i, \\gamma^{-1})$ where here $\\gamma \\in \\mathbb{R}$ is a constant real number and $\\mathcal{N}(\\mathbf{w}^T \\mathbf{x}_i, \\gamma^{-1})$ is the gaussian distribution with mean  $\\mathbf{w}^T \\mathbf{x}_i$ and variance $\\gamma^{-1}$.\n",
    "\n",
    "\n",
    "7- **2pt** Derive and express in vector/matrix form as a function of $\\mathbf{X}, \\mathbf{y}, \\mathbf{w}$ the log posterior $\\ln p(\\mathbf{w} | \\mathbf{y}, \\mathbf{X})$. Show all the derivations. You can ignore normalizing constants.\n",
    " \n",
    "\n",
    "\n",
    "8-  **2pt** Show that maximizing the posterior in 7- is  similar to minimizing the function $\\mathcal{L}(\\mathbf{w})$ seen in the previous section. Show your derivations. (Note: You should show this without computing the maximum of the posterior. Instead, you should express the log posterior in term of $\\mathcal{L}(\\mathbf{w})$, ignoring constants if necessary. Then find the $\\alpha$ of $\\mathcal{L}(\\mathbf{w})$ in term of $\\lambda$ and $\\gamma$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7** By Bayes' rule, we have\n",
    "\\begin{align}\n",
    "p(\\mathbf{w|y,X}) &= \\frac{p(\\mathbf{y|w,X})p(\\mathbf{w})}{p(\\mathbf{y|X})} \\\\\n",
    " &= \\frac{p(\\mathbf{y|w,X})p(\\mathbf{w})}{\\int p(\\mathbf{y|w,X})p(\\mathbf{w})\\mathrm{d}\\mathbf{w}}\n",
    "\\end{align}\n",
    "Ignoring normalizing constants, we have\n",
    "\\begin{align}\n",
    "p(\\mathbf{w|y,X}) &\\propto p(\\mathbf{y|w,X})p(\\mathbf{w}) \\\\\n",
    " &= \\mathcal{N}(\\mathbf{Xw}, \\gamma^{-1}\\mathbf{I}) \\cdot \\mathcal{N}(\\mathbf{0}, \\lambda^{-1}\\mathbf{I}) \\\\\n",
    " &= \\frac{1}{(2\\pi)^{N/2}|\\gamma^{-1}\\mathbf{I}|^{1/2}}exp(-\\frac{1}{2}(\\mathbf{y - Xw})^T(\\gamma^{-1}\\mathbf{I})^{-1}(\\mathbf{y - Xw})) \\\\\n",
    " &\\times \\frac{1}{(2\\pi)^{N/2}|\\lambda^{-1}\\mathbf{I}|^{1/2}}exp(-\\frac{1}{2}\\mathbf{w}^T(\\lambda^{-1}\\mathbf{I})^{-1}\\mathbf{w})\n",
    "\\end{align}\n",
    "Taking logarithm, and ignoring constants, we have\n",
    "\\begin{align}\n",
    "log(p(\\mathbf{w|y,X})) &\\propto -\\frac{1}{2}(\\mathbf{y - Xw})^T(\\gamma^{-1}\\mathbf{I})^{-1}(\\mathbf{y - Xw})-\\frac{1}{2}\\mathbf{w}^T(\\lambda^{-1}\\mathbf{I})^{-1}\\mathbf{w} \\\\\n",
    " &= -\\frac{\\gamma}{2}(\\mathbf{y - Xw})^T(\\mathbf{y - Xw})-\\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8** We have known that\n",
    "$$\\mathcal{L}(\\mathbf{w}) = (\\mathbf{y} - \\mathbf{Xw})^T(\\mathbf{y} - \\mathbf{Xw}) + \\alpha\\cdot\\mathbf{w}^T\\mathbf{w}$$\n",
    ", and from 7 we know\n",
    "\\begin{align}\n",
    "log(p(\\mathbf{w|y,X})) &\\propto-\\frac{\\gamma}{2}(\\mathbf{y - Xw})^T(\\mathbf{y - Xw})-\\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w} \\\\\n",
    " &= -\\frac{\\gamma}{2}((\\mathbf{y - Xw})^T(\\mathbf{y - Xw}) + \\frac{\\lambda}{\\gamma}\\mathbf{w}^T\\mathbf{w}) \\\\\n",
    " &= -\\frac{\\gamma}{2}\\mathcal{L}(\\mathbf{w}),\\, \\alpha = \\frac{\\lambda}{\\gamma}\n",
    "\\end{align}\n",
    "From the above formula, let $\\lambda, \\gamma \\gt 0$, we can see that minimizing the function $\\mathcal{L}(\\mathbf{w})$ is actually maximizing the posterior in 7."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW2_2019.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
